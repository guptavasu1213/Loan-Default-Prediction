{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Default Prediction\n",
    "\n",
    "</Write something about the project/>\n",
    "- Describe the task\n",
    "- Describe the potential issues (imbalanced dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just easy line to do imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydot\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score as PS\n",
    "from sklearn.metrics import recall_score as RS\n",
    "from sklearn.metrics import f1_score as FS\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This gets the data\n",
    "app_data = pd.read_csv('dataset/application_data.csv') #App data is for their specific application to the data\n",
    "app_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(app_data.shape) #Just so we know the amount of data we're working with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This one is to determine what things we have to change\n",
    "app_data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to adjust 16 columns to be effectively used. Scikit has a built-in for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_data.agg(['count', 'size', 'nunique'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Identifying missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the percentage of the missing data in each column\n",
    "def missing_data(data):\n",
    "    total = data.isnull().sum() #Total of null values in each column\n",
    "    percent = (total / data.isnull().count()) * 100\n",
    "    unique = data.nunique() #Unique values in entire dataset\n",
    "    datatype = data.dtypes #Finding data types in entire dataset\n",
    "    return pd.concat([total, percent, unique, datatype], axis=1, keys=['Total', 'Percent', 'Unique', 'Data_Type']).sort_values(by = 'Percent', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metData = missing_data(app_data)\n",
    "metData.head(30) #Percent represents the percentage of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Removing features with high missing data\n",
    "This one is to get rid of columns with too much missing data. \n",
    "We used 56.4% as the cutoff in order to keep EXT_SOURCE_1 feature which is important. If the missing data in a column is greater than the cutoff, then we are getting rid of the feature.\n",
    "\n",
    "We chose 56.4% because EXT_SOURCE_1 was cut off there. We could've arbitrarily chosen any value and have had it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keptColumns = list(metData[(metData.Percent<56.4)].index)\n",
    "appData = app_data[keptColumns]\n",
    "appData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing the extra features, we go forward with using 99/122 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Imputing low missing data\n",
    "\n",
    "For seeing the low missing data, cutoff chosen is 15%. This is just visualization. Fill in will be done in preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extraction of data from given range for edification\n",
    "lowMiss = pd.DataFrame(metData[(metData.Percent > 0) & (metData.Percent<15)])\n",
    "lowMiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Listing Out Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is so we know the data names for changing to numeric values.\n",
    "obs = appData.select_dtypes('object').columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floats = appData.select_dtypes('float').columns\n",
    "ints = appData.select_dtypes('int64').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking for just 2 unique values\n",
    "appData.select_dtypes('int64').apply(pd.Series.nunique, axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Converting Flag variables(0/1) to Categorical Variables(N/Y)\n",
    "Notice a lot of \"int\" columns have 2 unique values. These are flags or Flag type varibles. Which have no use in bivariate analysis. These can be converted to Yes/No values for categorical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns with 2 unique values and data type as 'int64'\n",
    "cols_to_convert=list(metData[(metData.Unique==2)&(metData.Data_Type==\"int64\")].index)\n",
    "appData[cols_to_convert] #Shows the columns we have to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts the data types of the columns form (0, 1) to ('N', 'Y')\n",
    "def convert_data(application_data, cols_to_convert):\n",
    "    for y in cols_to_convert:\n",
    "        application_data.loc[:,y].replace((0, 1), ('N', 'Y'), inplace=True)\n",
    "    return application_data\n",
    "\n",
    "#calling the function for application_data\n",
    "appData = convert_data(appData, cols_to_convert)\n",
    "appData[cols_to_convert] #Shows the columns after conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Changing the target label from (0,1) to (N/Y)\n",
    "\n",
    "We must change target back to its original state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting 'TARGET' label column from (0,1) to ('N','Y')\n",
    "appData.TARGET.replace(('N', 'Y'), (0, 1), inplace=True)\n",
    "appData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDefaulterAndNondefaulters(data):\n",
    "    defaulter = data[data.TARGET==1] #These are who did default\n",
    "    nonDefaulter = data[data.TARGET==0] #These are who didn't default\n",
    "    return defaulter, nonDefaulter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaulter, nonDefaulter = getDefaulterAndNondefaulters(appData)\n",
    "print(defaulter.shape, nonDefaulter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data Visualization\n",
    "\n",
    "Here we analyze the data by plotting certain features for defaulter and non-defaulters to compare and gain an insight to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Plotting Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got this from machine learning kernels on kaggle\n",
    "# https://www.kaggle.com/snehac47/credit-application-case-study\n",
    "\n",
    "def plot_charts(var, label_rotation,horizontal_layout):\n",
    "    if(horizontal_layout):\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,5))\n",
    "    else:\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(15,20))\n",
    "    \n",
    "    s1=sns.countplot(ax=ax1,x=defaulter[var], data=defaulter, order= defaulter[var].value_counts().index,)\n",
    "    ax1.set_title('Distribution of '+ '%s' %var +' for Defaulters', fontsize=10)\n",
    "    ax1.set_xlabel('%s' %var)\n",
    "    ax1.set_ylabel(\"Count of Loans\")\n",
    "    if(label_rotation):\n",
    "        s1.set_xticklabels(s1.get_xticklabels(),rotation=90)\n",
    "    s2=sns.countplot(ax=ax2,x=nonDefaulter[var], data=nonDefaulter, order= nonDefaulter[var].value_counts().index,)\n",
    "    if(label_rotation):\n",
    "        s2.set_xticklabels(s2.get_xticklabels(),rotation=90)\n",
    "    ax2.set_xlabel('%s' %var)\n",
    "    ax2.set_ylabel(\"Count of Loans\")\n",
    "    ax2.set_title('Distribution of '+ '%s' %var +' for Non-Defaulters', fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, more individuals and organizations are looking for cash loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts('NAME_CONTRACT_TYPE', False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More women were looking for loans than men."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts('CODE_GENDER', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts('FLAG_OWN_REALTY', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts('FLAG_OWN_CAR', False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts('NAME_HOUSING_TYPE', True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of people who live with parents and default is higher than the non-defaulters. This could be correlated with age, we'll have to look into age to see if that ratio remains similar, as it might just be an insight that younger people tend to default more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts('NAME_HOUSING_TYPE', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has 'DAYS_BIRTH' as a negative number representing the number of days since the birth of the user \n",
    "appData['AGE'] = appData['DAYS_BIRTH'] / -365\n",
    "plt.hist(appData['AGE'])\n",
    "plt.title('Age in years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to see who is looking for loans, as well as what types of businesses are looking for loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts('OCCUPATION_TYPE', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts('ORGANIZATION_TYPE', True, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting histogram and box plot for a given feature to the function\n",
    "def plot_boxhist(var):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    s=sns.boxplot(y=defaulters[var]);\n",
    "    plt.title('Box Plot of '+ '%s' %var +' for Defaulters', fontsize=10)\n",
    "    plt.xlabel('%s' %var)\n",
    "    plt.ylabel(\"Count of Loans\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    s=plt.hist(x=defaulters[var]);\n",
    "    plt.xlabel('%s' %var)\n",
    "    plt.ylabel(\"Count of Loans\")\n",
    "    plt.title('Histogram of '+ '%s' %var +' for Defaulters', fontsize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all entries above 99 percentile\n",
    "appData=appData[appData.AMT_INCOME_TOTAL<np.nanpercentile(appData['AMT_INCOME_TOTAL'], 99)]\n",
    "#update defaulters and non-defaulters after outlier removal\n",
    "defaulters, nondefaulters = getDefaulterAndNondefaulters(appData)\n",
    "\n",
    "# Graphing after the outlier removal\n",
    "plot_boxhist('AMT_INCOME_TOTAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaulter.loc[:,'INCOME_BRACKET']=pd.qcut(appData.loc[:,'AMT_INCOME_TOTAL'],q=[0,0.10,0.35,0.50,0.90,1], labels=['Very_low','Low','Medium','High','Very_high'])\n",
    "nonDefaulter.loc[:,'INCOME_BRACKET']=pd.qcut(appData.loc[:,'AMT_INCOME_TOTAL'],q=[0,0.10,0.35,0.50,0.90,1], labels=['Very_low','Low','Medium','High','Very_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_charts('INCOME_BRACKET', True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, everything follows about the same patter, but with more low income people defaulting, and more very high income people not defaulting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Label Encoding\n",
    "The labels are changed from ...##########??? \n",
    "The numerical missing data is filled with the mean values of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############???? Documentation?\n",
    "def dataSetup(appData):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for CName in appData.drop('TARGET', axis = 1).columns:\n",
    "        if type(appData[CName][1]) == str:\n",
    "            appData[CName] = appData[CName].fillna(\"Unknown\")\n",
    "        else:\n",
    "            appData[CName] = appData[CName].fillna(appData[CName].mean(), inplace=True)\n",
    "        \n",
    "        if CName == 'TARGET': pass\n",
    "        else:\n",
    "            appData[CName] = le.fit_transform(appData[CName])\n",
    "    return appData, appData['TARGET'], appData.drop('TARGET', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalAppData, labels, feats= dataSetup(appData)\n",
    "defaulters, nondefaulters = getDefaulterAndNondefaulters(finalAppData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.array(feats)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Creation (without dealing with data imbalancement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.#### Learning Curve\n",
    "Creating learning curve for the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.dataquest.io/blog/learning-curves-machine-learning/\n",
    "def draw_learning_curve(model, x, labels, cv, numTrainSizes, modelName):\n",
    "    trainingSetSize = len(x)*0.66 #Training set is 66% always\n",
    "    train_sizes = []\n",
    "    for i in range(numTrainSizes):\n",
    "        train_sizes.append((i+1)*int(trainingSetSize//numTrainSizes))\n",
    "    print(\"Training sizes\", train_sizes)\n",
    "    \n",
    "    #Feature scaling\n",
    "    std_scaler = StandardScaler()    \n",
    "    xScaled = std_scaler.fit_transform(x)    \n",
    "    \n",
    "    train_sizes, train_scores, validation_scores = learning_curve(estimator=model, X=xScaled, y=labels, cv=cv, train_sizes=train_sizes, scoring='neg_mean_squared_error') \n",
    "#     print(\"Val scores:\", validation_scores)\n",
    "#     print(\"Train Scores:\", train_scores)\n",
    "    \n",
    "    train_scores_mean = -train_scores.mean(axis = 1)\n",
    "    validation_scores_mean = -validation_scores.mean(axis = 1)\n",
    "    print('Mean training scores\\n\\n', pd.Series(train_scores_mean, index = train_sizes))\n",
    "    print('\\n', '-' * 20) # separator\n",
    "    print('\\nMean validation scores\\n\\n',pd.Series(validation_scores_mean, index = train_sizes))\n",
    "\n",
    "    plt.style.use('seaborn')\n",
    "    plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
    "    plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
    "    plt.ylabel('MSE', fontsize = 14)\n",
    "    plt.xlabel('Training set size', fontsize = 14)\n",
    "    plt.title(str('Learning curves for '+modelName+' model'), fontsize = 18, y = 1.03)\n",
    "    plt.legend()\n",
    "    return plt\n",
    "    # plt.xlim(0,10000)\n",
    "    # plt.ylim(0.85,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndPredictRandomForest(xTrain, yTrain, xTest, yTest):\n",
    "    #Feature scaling\n",
    "    std_scaler = StandardScaler()    \n",
    "    xTrain = std_scaler.fit_transform(xTrain)\n",
    "    xTest = std_scaler.transform(xTest)\n",
    "    \n",
    "    # Model training\n",
    "    rf = RandomForestClassifier(n_estimators = 1000, max_depth = 25)\n",
    "    rf.fit(xTrain, yTrain)\n",
    "    \n",
    "    # Model predicting \n",
    "    predictions = rf.predict(xTest)\n",
    "    print(\"Prediction:\" , np.average(predictions == yTest))\n",
    "    return predictions, rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the prediction of the test data and the model itself\n",
    "def trainAndPredictLogisticReg(xTrain, yTrain, xTest, yTest):\n",
    "    #Feature scaling\n",
    "    std_scaler = StandardScaler()    \n",
    "    xTrain = std_scaler.fit_transform(xTrain)\n",
    "    xTest = std_scaler.transform(xTest)\n",
    "    \n",
    "    # Model training\n",
    "    reg = LogisticRegression(random_state = 27).fit(xTrain, yTrain)\n",
    "    \n",
    "    # Model predicting\n",
    "    prediction = reg.predict(xTest)\n",
    "    print(\"Prediction:\" , np.average(prediction == yTest))\n",
    "    return prediction, reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the prediction of the test data and the model itself\n",
    "def trainAndPredictSVM(xTrain, yTrain, xTest, yTest):\n",
    "    #Feature scaling\n",
    "    std_scaler = StandardScaler()    \n",
    "    xTrain = std_scaler.fit_transform(xTrain)\n",
    "    xTest = std_scaler.transform(xTest)\n",
    "    \n",
    "    # Model training\n",
    "    linearSVM = svm.LinearSVC(C=1, max_iter=1000)\n",
    "    linearSVM.fit(xTrain, yTrain)\n",
    "    \n",
    "    # Model predicting\n",
    "    prediction = linearSVM.predict(xTest)\n",
    "    print(\"Prediction:\" , np.average(prediction == yTest))\n",
    "    return prediction, linearSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Cross Validation\n",
    "\n",
    "################????? EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit as sss\n",
    "# Creating 3 splits\n",
    "stratifiedShuffSplitter = sss(n_splits=3, test_size = .33, random_state = 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Evaluating the Models by calculating Precision, Recall, F1 score and confusion matrix\n",
    "def evaluateModel(yActual, yPredicted):\n",
    "    print(np.average(yPredicted == yActual), \"Average Correct\")\n",
    "    print(PS(yActual, yPredicted, average='micro'), \"Number for Precision score\")\n",
    "    print(RS(yActual, yPredicted, average='micro'), \"Number for Recall Score\")\n",
    "    print(FS(yActual, yPredicted, average='micro'), \"Number for F1 score\")\n",
    "    \n",
    "    #Plotting the confusion matrix\n",
    "    confusionMat = confusion_matrix(yActual, yPredicted)\n",
    "    np.fill_diagonal(confusionMat, 0)\n",
    "    plt.matshow(confusionMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Creating the Model\n",
    "# - Running Predictions \n",
    "# - Evaluating the Models by calculating Precision, Recall, F1 score and confusion matrix\n",
    "# - Cross validating the model with the number of folds specified in the shuffler\n",
    "def createAndCrossRandomForest(xData, labels, stratifiedShuffSplitter):\n",
    "    counter = 0\n",
    "    # Running the cross validation on the number of sets specified on the \n",
    "    # splitter\n",
    "    for train_index, test_index in stratifiedShuffSplitter.split(xData, labels):\n",
    "        counter += 1\n",
    "        print(\"\\n=============\\nRunning the\", counter, \"set\")\n",
    "        print(train_index.shape, test_index.shape)        \n",
    "        xTrain, yTrain = xData[train_index], labels[train_index]\n",
    "        xTest, yTest = xData[test_index], labels[test_index]\n",
    "\n",
    "        #Feature scaling\n",
    "        std_scaler = StandardScaler()    \n",
    "        xTrain = std_scaler.fit_transform(xTrain)\n",
    "        xTest = std_scaler.transform(xTest)\n",
    "        \n",
    "        # Training and predicting Random forest\n",
    "        print(\"\\nRunning Random Forest\")\n",
    "        rf = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "        rf.fit(xTrain, yTrain)\n",
    "        predictions = rf.predict(xTest)\n",
    "        draw_learning_curve()\n",
    "        evaluateModel(yTest, predictions) \n",
    "        plt.show()\n",
    "        \n",
    "        model = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "        draw_learning_curve(model=model, x=xData, labels=labels, cv=5, numTrainSizes=20, modelName=str(\"Random Forest Split Num:\"+str(counter)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Creating the Models\n",
    "# - Running Predictions \n",
    "# - Evaluating the Models by calculating Precision, Recall, F1 score and confusion matrix\n",
    "# - Cross validating the model with the number of folds specified in the shuffler\n",
    "def createAndCrossLogisticReg(xData, labels, stratifiedShuffSplitter):\n",
    "    counter = 0\n",
    "    # Running the cross validation on the number of sets specified on the \n",
    "    # splitter\n",
    "    for train_index, test_index in stratifiedShuffSplitter.split(xData, labels):\n",
    "        counter += 1\n",
    "        print(\"\\n\\n Running the\", counter, \"set\")\n",
    "        print(train_index.shape, test_index.shape)        \n",
    "        xTrain, yTrain = xData[train_index], labels[train_index]\n",
    "        xTest, yTest = xData[test_index], labels[test_index]\n",
    "\n",
    "        #Feature scaling\n",
    "        std_scaler = StandardScaler()    \n",
    "        xTrain = std_scaler.fit_transform(xTrain)\n",
    "        xTest = std_scaler.transform(xTest)\n",
    "        \n",
    "        # Training and predicting Logistic Regression\n",
    "        print(\"\\nRunning Logistic Regression\")\n",
    "        reg = LogisticRegression(random_state = 0).fit(xTrain, yTrain)\n",
    "        predictions = reg.predict(xTest)\n",
    "        evaluateModel(yTest, predictions)\n",
    "        plt.show()\n",
    "        \n",
    "        model = LogisticRegression(random_state = 0)\n",
    "        draw_learning_curve(model=model, x=xData, labels=labels, cv=5, numTrainSizes=20, modelName=str(\"Logistic Regression Split Num:\"+str(counter)))        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Creating the Models\n",
    "# - Running Predictions \n",
    "# - Evaluating the Models by calculating Precision, Recall, F1 score and confusion matrix\n",
    "# - Cross validating the model with the number of folds specified in the shuffler\n",
    "def createAndCrossSVM(xData, labels, stratifiedShuffSplitter):\n",
    "    counter = 0\n",
    "    # Running the cross validation on the number of sets specified on the \n",
    "    # splitter\n",
    "    for train_index, test_index in stratifiedShuffSplitter.split(xData, labels):\n",
    "        counter += 1\n",
    "        print(\"\\n\\n Running the\", counter, \"set\")\n",
    "        print(train_index.shape, test_index.shape)        \n",
    "        xTrain, yTrain = xData[train_index], labels[train_index]\n",
    "        xTest, yTest = xData[test_index], labels[test_index]\n",
    "\n",
    "        #Feature scaling\n",
    "        std_scaler = StandardScaler()    \n",
    "        xTrain = std_scaler.fit_transform(xTrain)\n",
    "        xTest = std_scaler.transform(xTest)\n",
    "        \n",
    "        print(\"\\nRunning SVM\")\n",
    "        # Model training\n",
    "        linearSVM = svm.LinearSVC(C=1, max_iter=1000)\n",
    "        linearSVM.fit(xTrain, yTrain)\n",
    "\n",
    "        # Model predicting\n",
    "        predictions = linearSVM.predict(xTest)\n",
    "        evaluateModel(yTest, predictions)\n",
    "        plt.show()\n",
    "        \n",
    "        model = svm.LinearSVC(C=1, max_iter=1000)\n",
    "        draw_learning_curve(model=model, x=xData, labels=labels, cv=5, numTrainSizes=20, modelName=str(\"SVM Split Num:\"+str(counter)))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.1 Cross validation with all filtered features\n",
    "Creating and evaluating the models with all the 99 filtered features after the data cleanup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createAndCrossRandomForest(feats, labels, stratifiedShuffSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createAndCrossLogisticReg(feats, labels, stratifiedShuffSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createAndCrossSVM(feats, labels, stratifiedShuffSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 Cross validation with reduced dimensionality using PCA\n",
    "\n",
    "##??? Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projData(myX, myU, K):\n",
    "    return myX @ myU[:, :K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loweredDimension = 30 #Reducing to features \n",
    "\n",
    "scaler = StandardScaler() #Set up scaler\n",
    "xScaled = scaler.fit_transform(feats) #Have to scale features for PCA to work\n",
    "\n",
    "covariance = (1 / xScaled.shape[0]) * xScaled.T @ xScaled #Calculate the covariance matrix for Singular Value decomp\n",
    "U, S, V = sp.linalg.svd(covariance)\n",
    "\n",
    "xReduced = projData(xScaled, U, loweredDimension) #Projects data into lower dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and evaluating models with reduced dimensionality\n",
    "# createAndCrossRandomForest(xReduced, labels, stratifiedShuffSplitter)\n",
    "# createAndCrossLogisticReg(xReduced, labels, stratifiedShuffSplitter)\n",
    "# createAndCrossSVM(xReduced, labels, stratifiedShuffSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 Cross validation using important features\n",
    "The models are created using the important data found using Random Forest and LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Finding important features from random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the train the samples from the sss\n",
    "stratifiedShuffSplitterRF = sss(n_splits=1, test_size = .33, random_state = 27)\n",
    "\n",
    "# Running the cross validation on the number of sets specified on the \n",
    "# splitter\n",
    "for train_indices, test_indices in stratifiedShuffSplitterRF.split(feats, labels):\n",
    "    xTrain, yTrain = feats[train_indices], labels[train_indices]\n",
    "    xTest, yTest = feats[test_indices], labels[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RF on the test and train data.\n",
    "predictions, rf = trainAndPredictRandomForest(xTrain, yTrain, xTest, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Finding important features from Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LR on the test and train data.\n",
    "predictions, lr = trainAndPredictLogisticReg(xTrain, yTrain, xTest, yTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying important data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the important features of RF\n",
    "RFimportances = rf.feature_importances_\n",
    "# print(RFimportances)\n",
    "\n",
    "# Find the important features of LR\n",
    "LRimportances = lr.coef_[0]\n",
    "# print(LRimportances)\n",
    "        \n",
    "# Remove the less important features from the original dataset\n",
    "colsToDelete = []\n",
    "for i in range(len(RFimportances)):\n",
    "    if RFimportances[i] == 0 or LRimportances[i] == 0:\n",
    "        colsToDelete.append(i)\n",
    "impFeats = np.delete(feats, colsToDelete, axis=1)\n",
    "print(impFeats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 46 features which have weight towards the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createAndCrossRandomForest(impFeats, labels, stratifiedShuffSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createAndCrossLogisticReg(impFeats, labels, stratifiedShuffSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createAndCrossSVM(impFeats, labels, stratifiedShuffSplitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Creation (Dealing with data imbalancement)\n",
    "\n",
    "Most of the machine learning algorithms work the best when the samples in each class are about equal. \n",
    "\n",
    "In case of our data set, we have 307,511 total entries out of which 282,686 values are non-defaulters and only 24,825 entries are of defaulters. So the minority class is only about 8% of the entire data set.\n",
    "\n",
    "There are different techniques which resolve the issue of over sampling. All these techniques are implemented below. (Source: https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Identifying defaulters and non-defaulters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.1 Extracting Defaulters and Non Defaulters using filtered data\n",
    "Using the filtered data after data cleanup, the defaulters and non defaulters were identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredDefaulters, filteredNonDefaulters = getDefaulterAndNondefaulters(finalAppData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.2 Extracting Defaulters and Non Defaulters using important features\n",
    "Using the data after finding the important features through Random Forest and Logistic Regression Models, the defaulters and non defaulters were identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data of important features from the dataframe\n",
    "colNames = finalAppData.drop('TARGET', axis= 1).columns[colsToDelete]\n",
    "importantData = finalAppData.drop(colNames, axis=1) #Drop all the irrelevant columns   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data for defaulter and nondefaulter classes from the important features only \n",
    "impDefaulters, impNonDefaulters = getDefaulterAndNondefaulters(importantData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 Extracting Defaulters and Non Defaulters using PCA data\n",
    "Using the reduced data by running PCA, the defaulters and Non defaulters are determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pandas dataframe for the reduced data by PCA\n",
    "reducedDataDict = {}\n",
    "#Creating the dictionary for the columns\n",
    "for i in range(loweredDimension):\n",
    "    reducedDataDict[\"C\"+str(i+1)] = xReduced[:,i]\n",
    "reducedDataDict[\"TARGET\"] = labels\n",
    "\n",
    "pcaData = pd.DataFrame(reducedDataDict, columns=list(reducedDataDict.keys()))\n",
    "pcaData.head(10)\n",
    "\n",
    "pcaDefaulters, pcaNonDefaulters = getDefaulterAndNondefaulters(pcaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into Labels and Features\n",
    "def getFeaturesAndLabels(data):\n",
    "    labels = data.TARGET\n",
    "    features = data.drop('TARGET', axis = 1)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Undersampling Majority Class\n",
    "Undersampling is a method of removing data from the majority class. This method is a good choice when we have a huge dataset.\n",
    "\n",
    "Drawback- We might remove valuable data which might lead to an underfit and poor generalization to the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downsampling the majority class\n",
    "def downsampleMajority(non_defaulter, defaulter):    \n",
    "    non_defaulters_downsampled = resample(non_defaulter,\n",
    "                                         replace = False, # resample without replacement\n",
    "                                         n_samples = len(defaulter), # Minority Class\n",
    "                                         random_state = 27) # reproducible results\n",
    "\n",
    "    # Combining minority and downsampled majority class\n",
    "    downsampledData = pd.concat([non_defaulters_downsampled, defaulter])\n",
    "    return downsampledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the test and train split from the downsampled data\n",
    "def getTestTrainDownsampled(downsampledData):\n",
    "    # Extracting the features and labels\n",
    "    downSampledFeatures, downSampledLabels = getFeaturesAndLabels(downsampledData)\n",
    "\n",
    "    # Split the data into test and train\n",
    "    trainFeatUnderSamp, testFeatUnderSamp, trainLabelsUnderSamp, testLabelsUnderSamp = train_test_split(downSampledFeatures, downSampledLabels, test_size=0.33, random_state=27)\n",
    "    return trainFeatUnderSamp, testFeatUnderSamp, trainLabelsUnderSamp, testLabelsUnderSamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Undersampling using filtered features\n",
    "Using the data after peforming the data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying counts for Defaulter and Non-Defaulter classes\n",
    "filDownSampledData = downsampleMajority(filteredNonDefaulters, filteredDefaulters)\n",
    "filDownSampledData.TARGET.value_counts()\n",
    "\n",
    "# Getting the undersampled train data and test data\n",
    "xTrainUnderSamp, xTestUnderSamp, yTrainUnderSamp, yTestUnderSamp = getTestTrainDownsampled(filDownSampledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Running logistic regression for undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0.5844291507926754\n",
      "0.5844291507926754 Average Correct\n",
      "0.5844291507926754 Number for Precision score\n",
      "0.5844291507926754 Number for Recall Score\n",
      "0.5844291507926754 Number for F1 score\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 396x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training and Predicting Logistic Regression Model\n",
    "predictions, lr = trainAndPredictLogisticReg(xTrainUnderSamp, yTrainUnderSamp, xTestUnderSamp, yTestUnderSamp)\n",
    "evaluateModel(yTestUnderSamp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sizes [3254, 6508, 9762, 13016, 16270, 19524, 22778, 26032, 29286, 32540]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n",
      "F:\\Downloads\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training scores\n",
      "\n",
      " 3254          NaN\n",
      "6508          NaN\n",
      "9762          NaN\n",
      "13016         NaN\n",
      "16270         NaN\n",
      "19524         NaN\n",
      "22778    0.134015\n",
      "26032    0.242163\n",
      "29286    0.319941\n",
      "32540    0.370602\n",
      "dtype: float64\n",
      "\n",
      " --------------------\n",
      "\n",
      "Mean validation scores\n",
      "\n",
      " 3254          NaN\n",
      "6508          NaN\n",
      "9762          NaN\n",
      "13016         NaN\n",
      "16270         NaN\n",
      "19524         NaN\n",
      "22778    0.499980\n",
      "26032    0.497141\n",
      "29286    0.473192\n",
      "32540    0.439976\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(filDownSampledData)\n",
    "model = LogisticRegression(random_state = 27)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=10, modelName=\"Logistic Regression Undersampled (filtered features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM \n",
    "Running SVM for undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, lr = trainAndPredictSVM(xTrainUnderSamp, yTrainUnderSamp, xTestUnderSamp, yTestUnderSamp)\n",
    "evaluateModel(yTestUnderSamp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(filDownSampledData)\n",
    "model = svm.LinearSVC(C=1, max_iter=1000)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=10, modelName=\"SVM Undersampled (filtered features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Running Random forest for undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Random Forest Model\n",
    "predictions, rf = trainAndPredictRandomForest(xTrainUnderSamp, yTrainUnderSamp, xTestUnderSamp, yTestUnderSamp)\n",
    "evaluateModel(yTestUnderSamp, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(filDownSampledData)\n",
    "model = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=10, modelName=\"Random Forest Undersampled (filtered features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Undersampling using important features data\n",
    "The important data found through Logistic Regression and Random Forest is only used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying counts for Defaulter and Non-Defaulter classes\n",
    "impDownSampledData = downsampleMajority(impNonDefaulters, impDefaulters)\n",
    "impDownSampledData.TARGET.value_counts()\n",
    "\n",
    "# Getting the undersampled train data and test data\n",
    "xTrainUnderSamp, xTestUnderSamp, yTrainUnderSamp, yTestUnderSamp = getTestTrainDownsampled(impDownSampledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Running logistic regression for undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Logistic Regression Model\n",
    "predictions, lr = trainAndPredictLogisticReg(xTrainUnderSamp, yTrainUnderSamp, xTestUnderSamp, yTestUnderSamp)\n",
    "evaluateModel(yTestUnderSamp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(impDownSampledData)\n",
    "model = LogisticRegression(random_state = 27)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Logistic Regression Undersampled (important features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM \n",
    "Running SVM for undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, lr = trainAndPredictSVM(xTrainUnderSamp, yTrainUnderSamp, xTestUnderSamp, yTestUnderSamp)\n",
    "evaluateModel(yTestUnderSamp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(impDownSampledData)\n",
    "model = svm.LinearSVC(C=1, max_iter=1000)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"SVM Undersampled (important features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Running Random forest for undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Random Forest Model\n",
    "predictions, rf = trainAndPredictRandomForest(xTrainUnderSamp, yTrainUnderSamp, xTestUnderSamp, yTestUnderSamp)\n",
    "evaluateModel(yTestUnderSamp, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(impDownSampledData)\n",
    "model = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Random Forest Undersampled (important features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 Undersampling using PCA data\n",
    "The reduced data after running PCA is used for performing undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying counts for Defaulter and Non-Defaulter classes\n",
    "pcaDownSampledData = downsampleMajority(pcaNonDefaulters, pcaDefaulters)\n",
    "pcaDownSampledData.TARGET.value_counts()\n",
    "\n",
    "# Getting the undersampled train data and test data\n",
    "xTrainUnderSamp, xTestUnderSamp, yTrainUnderSamp, yTestUnderSamp = getTestTrainDownsampled(pcaDownSampledData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Running logistic regression for undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Logistic Regression Model\n",
    "predictions, lr = trainAndPredictLogisticReg(xTrainUnderSamp, yTrainUnderSamp, xTestUnderSamp, yTestUnderSamp)\n",
    "evaluateModel(yTestUnderSamp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(pcaDownSampledData)\n",
    "model = LogisticRegression(random_state = 27)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Logistic Regression Undersampled (PCA features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM \n",
    "Running SVM for undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, lr = trainAndPredictSVM(xTrainUnderSamp, yTrainUnderSamp, xTestUnderSamp, yTestUnderSamp)\n",
    "evaluateModel(yTestUnderSamp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(pcaDownSampledData)\n",
    "model = svm.LinearSVC(C=1, max_iter=1000)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"SVM Undersampled (PCA features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Running Random forest for undersampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Random Forest Model\n",
    "predictions, rf = trainAndPredictRandomForest(xTrainUnderSamp, yTrainUnderSamp, xTestUnderSamp, yTestUnderSamp)\n",
    "evaluateModel(yTestUnderSamp, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(pcaDownSampledData)\n",
    "model = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Random Forest Undersampled (PCA features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Oversampling Minority Class\n",
    "\n",
    "Oversampling is a method of adding duplicates from the minority class to make it equal to the size of the majority class.\n",
    "\n",
    "Oversampling is usually used when there is not a lot of data to work with. But let's try and see what happens in our dataset.\n",
    "\n",
    "(Source: https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)\n",
    "\n",
    "Important Note:\n",
    "We should always seperate the original dataset into test and train sets before performing oversampling. This is because the repetition in the data can allow the model to memorize specific data points which can lead to an overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestTrainOversampled(data):    \n",
    "    # Split the data into features and labels\n",
    "    feats, labels = getFeaturesAndLabels(data)\n",
    "    \n",
    "    #Setting up the test and train data\n",
    "    trainFeatOverSamp, testFeatOverSamp, trainLabelsOverSamp, testLabelsOverSamp = train_test_split(feats, labels, test_size=0.33, random_state=27)\n",
    "\n",
    "    # Concatenate the training features and the labels \n",
    "    trainData = pd.concat([trainFeatOverSamp, trainLabelsOverSamp], axis = 1)    \n",
    "    \n",
    "    # Seperate the majority and the minority classes from the Train Dataset\n",
    "    trainDefaulters    = trainData[trainData.TARGET == 1] \n",
    "    trainNonDefaulters = trainData[trainData.TARGET == 0]\n",
    "    \n",
    "    # Upsampling the minority class in the train data\n",
    "    defaulters_upsampled = resample(trainDefaulters, # the dataset to scales up\n",
    "                                   replace = True, #resample with replacement\n",
    "                                    n_samples = len(trainNonDefaulters), #Num of majority class entries\n",
    "                                    random_state = 27) \n",
    "    upsampledData = pd.concat([trainNonDefaulters, defaulters_upsampled]) \n",
    "\n",
    "    #Verifying the number of entries for each class\n",
    "    upsampledData.TARGET.value_counts()\n",
    "\n",
    "    #Splitting the upsampled train data to features and labels\n",
    "    trainFeats, trainLabels = getFeaturesAndLabels(upsampledData)\n",
    "    return trainFeats, testFeatOverSamp, trainLabels, testLabelsOverSamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 Oversampling using filtered features\n",
    "Using the data after peforming the data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the undersampled train data and test data\n",
    "xTrainOverSamp, xTestOverSamp, yTrainOverSamp, yTestOverSamp = getTestTrainOversampled(finalAppData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Running logistic regression for oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Logistic Regression Model\n",
    "predictions, lr = trainAndPredictLogisticReg(xTrainOverSamp, yTrainOverSamp, xTestOverSamp, yTestOverSamp)\n",
    "evaluateModel(yTestOverSamp, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(finalAppData)\n",
    "model = LogisticRegression(random_state = 27)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Logistic Regression oversampled (Filtered features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM \n",
    "Running SVM for oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, lr = trainAndPredictSVM(xTrainOverSamp, yTrainOverSamp, xTestOverSamp, yTestOverSamp)\n",
    "evaluateModel(yTestOverSamp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(finalAppData)\n",
    "model = svm.LinearSVC(C=1, max_iter=1000)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"SVM oversampled (Filtered features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Running Random forest for oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Random Forest Model\n",
    "predictions, rf = trainAndPredictRandomForest(xTrainOverSamp, yTrainOverSamp, xTestOverSamp, yTestOverSamp)\n",
    "evaluateModel(yTestOverSamp, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(finalAppData)\n",
    "model = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Random Forest oversampled (Filtered features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 Oversampling using important features data\n",
    "The important data found through Logistic Regression and Random Forest is only used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the undersampled train data and test data\n",
    "xTrainOverSamp, xTestOverSamp, yTrainOverSamp, yTestOverSamp = getTestTrainOversampled(importantData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Running logistic regression for oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Logistic Regression Model\n",
    "predictions, lr = trainAndPredictLogisticReg(xTrainOverSamp, yTrainOverSamp, xTestOverSamp, yTestOverSamp)\n",
    "evaluateModel(yTestOverSamp, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(importantData)\n",
    "model = LogisticRegression(random_state = 27)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Logistic Regression oversampled (Important features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM \n",
    "Running SVM for oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, lr = trainAndPredictSVM(xTrainOverSamp, yTrainOverSamp, xTestOverSamp, yTestOverSamp)\n",
    "evaluateModel(yTestOverSamp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(importantData)\n",
    "model = svm.LinearSVC(C=1, max_iter=1000)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"SVM oversampled (Important features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Running Random forest for oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Random Forest Model\n",
    "predictions, rf = trainAndPredictRandomForest(xTrainOverSamp, yTrainOverSamp, xTestOverSamp, yTestOverSamp)\n",
    "evaluateModel(yTestOverSamp, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(importantData)\n",
    "model = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Random Forest oversampled (Important features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.3 Oversampling using PCA data\n",
    "The reduced data after running PCA is used for performing oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the undersampled train data and test data\n",
    "xTrainOverSamp, xTestOverSamp, yTrainOverSamp, yTestOverSamp = getTestTrainOversampled(pcaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Running logistic regression for oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Logistic Regression Model\n",
    "predictions, lr = trainAndPredictLogisticReg(xTrainOverSamp, yTrainOverSamp, xTestOverSamp, yTestOverSamp)\n",
    "evaluateModel(yTestOverSamp, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(pcaData)\n",
    "model = LogisticRegression(random_state = 27)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Logistic Regression oversampled (PCA features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM \n",
    "Running SVM for oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, lr = trainAndPredictSVM(xTrainOverSamp, yTrainOverSamp, xTestOverSamp, yTestOverSamp)\n",
    "evaluateModel(yTestOverSamp, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(pcaData)\n",
    "model = svm.LinearSVC(C=1, max_iter=1000)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"SVM oversampled (PCA features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Running Random forest for oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Random Forest Model\n",
    "predictions, rf = trainAndPredictRandomForest(xTrainOverSamp, yTrainOverSamp, xTestOverSamp, yTestOverSamp)\n",
    "evaluateModel(yTestOverSamp, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(pcaData)\n",
    "model = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Random Forest oversampled (PCA features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Synthetic Data\n",
    "\n",
    "SMOTE oversamples the minority class by using nearest neighbours algorithm and generates new synthetic data which is used for model training.\n",
    "\n",
    "Important note: Similar to data oversampling, the data is splitted into test and training set at the beginning to avoid repetition in the data that can allow the model to memorize specific data points which can lead to an overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestTrainSynthetic(data):\n",
    "    # Split the data into features and labels\n",
    "    x, y = getFeaturesAndLabels(data)\n",
    "\n",
    "    # setting up testing and training sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=27)\n",
    "\n",
    "    sm = SMOTE(random_state=27)\n",
    "    #Running SMOTE on training set \n",
    "    # Upsampling the minority defaulter class by generating new samples\n",
    "    X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 Generating synthetic data using filtered features\n",
    "Using the data after peforming the data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Random Forest Model\n",
    "predictions, rf = trainAndPredictRandomForest(xTrainOverSamp, yTrainOverSamp, xTestOverSamp, yTestOverSamp)\n",
    "evaluateModel(yTestOverSamp, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the train data and test data after creating synthetic data\n",
    "xTrainSynthetic, xTestSynthetic, yTrainSynthetic, yTestSynthetic = getTestTrainSynthetic(finalAppData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Running logistic regression for synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Logistic Regression Model\n",
    "predictions, lr = trainAndPredictLogisticReg(xTrainSynthetic, yTrainSynthetic, xTestSynthetic, yTestSynthetic)\n",
    "evaluateModel(yTestSynthetic, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(finalAppData)\n",
    "model = LogisticRegression(random_state = 27)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Logistic Regression synthetic (Filtered features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM \n",
    "Running SVM for synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, lr = trainAndPredictSVM(xTrainSynthetic, yTrainSynthetic, xTestSynthetic, yTestSynthetic)\n",
    "evaluateModel(yTestSynthetic, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(finalAppData)\n",
    "model = svm.LinearSVC(C=1, max_iter=1000)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"SVM synthetic (Filtered features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Running Random forest for synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Random Forest Model\n",
    "predictions, rf = trainAndPredictRandomForest(xTrainSynthetic, yTrainSynthetic, xTestSynthetic, yTestSynthetic)\n",
    "evaluateModel(yTestSynthetic, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(finalAppData)\n",
    "model = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Random forest synthetic (Filtered features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 Generating synthetic data using important features data\n",
    "The important data found through Logistic Regression and Random Forest is only used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the train data and test data after creating synthetic data\n",
    "xTrainSynthetic, xTestSynthetic, yTrainSynthetic, yTestSynthetic = getTestTrainSynthetic(importantData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Running logistic regression for synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Logistic Regression Model\n",
    "predictions, lr = trainAndPredictLogisticReg(xTrainSynthetic, yTrainSynthetic, xTestSynthetic, yTestSynthetic)\n",
    "evaluateModel(yTestSynthetic, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(importantData)\n",
    "model = LogisticRegression(random_state = 27)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Logistic Regression synthetic (Important features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM \n",
    "Running SVM for synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, lr = trainAndPredictSVM(xTrainSynthetic, yTrainSynthetic, xTestSynthetic, yTestSynthetic)\n",
    "evaluateModel(yTestSynthetic, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(importantData)\n",
    "model = svm.LinearSVC(C=1, max_iter=1000)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"SVM synthetic (Important features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Running Random forest for synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Random Forest Model\n",
    "predictions, rf = trainAndPredictRandomForest(xTrainSynthetic, yTrainSynthetic, xTestSynthetic, yTestSynthetic)\n",
    "evaluateModel(yTestSynthetic, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(importantData)\n",
    "model = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Random Forest synthetic (Important features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 Generating synthetic data using PCA data\n",
    "The reduced data after running PCA is used for generation of synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the train data and test data after creating synthetic data\n",
    "xTrainSynthetic, xTestSynthetic, yTrainSynthetic, yTestSynthetic = getTestTrainSynthetic(pcaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Running logistic regression for synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Logistic Regression Model\n",
    "predictions, lr = trainAndPredictLogisticReg(xTrainSynthetic, yTrainSynthetic, xTestSynthetic, yTestSynthetic)\n",
    "evaluateModel(yTestSynthetic, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(pcaData)\n",
    "model = LogisticRegression(random_state = 27)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Logistic Regression synthetic (PCA features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM \n",
    "Running SVM for synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, lr = trainAndPredictSVM(xTrainSynthetic, yTrainSynthetic, xTestSynthetic, yTestSynthetic)\n",
    "evaluateModel(yTestSynthetic, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(pcaData)\n",
    "model = svm.LinearSVC(C=1, max_iter=1000)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"SVM synthetic (PCA features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "Running Random forest for synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Predicting Random Forest Model\n",
    "predictions, rf = trainAndPredictRandomForest(xTrainSynthetic, yTrainSynthetic, xTestSynthetic, yTestSynthetic)\n",
    "evaluateModel(yTestSynthetic, predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the learning curve\n",
    "xData, yData = getFeaturesAndLabels(pcaData)\n",
    "model = RandomForestClassifier(n_estimators = 1000, max_depth = 10)\n",
    "draw_learning_curve(model=model, x=xData, labels=yData, cv=5, numTrainSizes=20, modelName=\"Random Forest synthetic (PCA features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
